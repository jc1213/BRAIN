{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54097892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import torch.nn.init as init\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65eaca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the concatenated images\n",
    "concat_directory = \"/home/yasmine/OASIS3/CNN/concat2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a70c7f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device for training (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\")\n",
    "# Print the device being used\n",
    "print(\"Device:\", device)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11d6d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)  # Assuming you have 4 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, num_patches, 1, 16, 16, 16)\n",
    "        batch_size, num_patches, _, _, _ = x.size()\n",
    "        x = x.view(batch_size * num_patches, 1, 16, 16, 16)\n",
    "        x = self.conv1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e76034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom dataset class\n",
    "class MRICTDataset(Dataset):\n",
    "    def __init__(self, data_directory, patch_size=(16, 16, 16), num_patches=200):\n",
    "        self.data_files = glob.glob(os.path.join(data_directory, \"*.nii.gz\"))\n",
    "        self.labels_df = pd.read_csv(\"/home/yasmine/OASIS3/CNN/hot_deck_labels_2.csv\")\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.resize_transform = Resize((16, 16))\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        # Fit and transform the string labels to numerical values\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels_df[\"Diagnosis\"])\n",
    "\n",
    "        # Create a dictionary to map subject IDs to labels\n",
    "        self.subject_labels = dict(zip(self.labels_df[\"Subject_ID\"], self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def extract_patches(self, image):\n",
    "        _, depth, height, width = image.shape\n",
    "        patches = []\n",
    "\n",
    "        for z in range(0, depth - self.patch_size[0] + 1, self.patch_size[0]):\n",
    "            for y in range(0, height - self.patch_size[1] + 1, self.patch_size[1]):\n",
    "                for x in range(0, width - self.patch_size[2] + 1, self.patch_size[2]):\n",
    "                    patch = image[:, z:z+self.patch_size[0], y:y+self.patch_size[1], x:x+self.patch_size[2]]\n",
    "                    patch_std = np.std(patch)\n",
    "                    patches.append((patch, patch_std))\n",
    "\n",
    "        # Sort patches based on information measure (from most info to less info)\n",
    "        patches.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Take the top 'num_patches' patches\n",
    "        selected_patches = [patch for patch, _ in patches[:self.num_patches]]\n",
    "\n",
    "        return selected_patches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file = self.data_files[index]\n",
    "        image = nib.load(file).get_fdata()\n",
    "        image = image.astype(np.float32)  # Convert image to NumPy array\n",
    "\n",
    "        # Ensure that the image has the correct shape (1, depth, height, width)\n",
    "        image = image[np.newaxis, ...]\n",
    "\n",
    "        # Extract and select patches\n",
    "        selected_patches = self.extract_patches(image)\n",
    "\n",
    "        # Convert selected_patches to NumPy arrays\n",
    "        selected_patches = np.array(selected_patches)\n",
    "\n",
    "        # Extract the subject ID from the file name\n",
    "        subject_id = os.path.splitext(os.path.basename(file))[0].split(\"_\")[0][11:]\n",
    "\n",
    "        # Retrieve the corresponding label from the dictionary\n",
    "        label = self.subject_labels.get(subject_id, 3)\n",
    "        #print(selected_patches.shape)\n",
    "\n",
    "        return selected_patches, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ce61a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "num_patches = 200\n",
    "# Split your dataset into training and validation sets\n",
    "# Directory containing the concatenated images\n",
    "concat_directory = \"/home/yasmine/OASIS3/CNN/concat2\"\n",
    "dataset = MRICTDataset(concat_directory)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = Conv3DModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "727e5f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████████████████████████████████| 34/34 [16:17<00:00, 28.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "Train Loss: 17.4819\n",
      "Train Accuracy: 38.99%\n",
      "Validation Loss: 5.2062\n",
      "Validation Accuracy: 51.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████████████████████████████████| 34/34 [44:27<00:00, 78.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]\n",
      "Train Loss: 4.2493\n",
      "Train Accuracy: 53.26%\n",
      "Validation Loss: 3.4074\n",
      "Validation Accuracy: 53.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|███████████████████████████████████| 34/34 [1:08:47<00:00, 121.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]\n",
      "Train Loss: 2.6071\n",
      "Train Accuracy: 59.24%\n",
      "Validation Loss: 3.0378\n",
      "Validation Accuracy: 56.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|███████████████████████████████████| 34/34 [1:07:19<00:00, 118.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]\n",
      "Train Loss: 2.3752\n",
      "Train Accuracy: 62.31%\n",
      "Validation Loss: 3.4824\n",
      "Validation Accuracy: 54.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|███████████████████████████████████| 34/34 [1:14:57<00:00, 132.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]\n",
      "Train Loss: 2.3013\n",
      "Train Accuracy: 61.75%\n",
      "Validation Loss: 2.8729\n",
      "Validation Accuracy: 53.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|███████████████████████████████████| 34/34 [1:14:04<00:00, 130.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]\n",
      "Train Loss: 2.0973\n",
      "Train Accuracy: 61.66%\n",
      "Validation Loss: 2.7971\n",
      "Validation Accuracy: 57.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|███████████████████████████████████| 34/34 [1:06:49<00:00, 117.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]\n",
      "Train Loss: 2.1567\n",
      "Train Accuracy: 61.94%\n",
      "Validation Loss: 4.3395\n",
      "Validation Accuracy: 53.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:  41%|███████████████▏                     | 14/34 [27:40<39:32, 118.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# Training loop with progress bar\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Wrap train_loader with tqdm to add the progress bar\n",
    "    for batch_data, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for each patch\n",
    "        patch_outputs = []\n",
    "        for patch_idx in range(num_patches):\n",
    "            current_patch = batch_data[:, patch_idx, :, :, :]\n",
    "            patch_output = model(current_patch)\n",
    "            patch_outputs.append(patch_output)\n",
    "\n",
    "        # Concatenate the patch outputs along the batch dimension\n",
    "        outputs = torch.cat(patch_outputs, dim=1)\n",
    "\n",
    "        # Compute loss for each subject (patch) separately\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    # Calculate and print training accuracy\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in val_loader:\n",
    "            # Forward pass for each patch\n",
    "            patch_outputs = []\n",
    "            for patch_idx in range(num_patches):\n",
    "                current_patch = batch_data[:, patch_idx, :, :, :]\n",
    "                patch_output = model(current_patch)\n",
    "                patch_outputs.append(patch_output)\n",
    "\n",
    "            # Concatenate the patch outputs along the batch dimension\n",
    "            outputs = torch.cat(patch_outputs, dim=1)\n",
    "\n",
    "            # Compute loss for each subject (patch) separately\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    # Calculate and print validation accuracy\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {total_loss / len(val_loader):.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# After training and validation, you can include a testing loop in a similar manner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop (similar to validation loop)\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        # Forward pass for each patch\n",
    "        patch_outputs = []\n",
    "        for patch_idx in range(num_patches):\n",
    "            current_patch = batch_data[:, patch_idx, :, :, :]\n",
    "            patch_output = model(current_patch)\n",
    "            patch_outputs.append(patch_output)\n",
    "\n",
    "        # Concatenate the patch outputs along the batch dimension\n",
    "        outputs = torch.cat(patch_outputs, dim=1)\n",
    "\n",
    "        # Compute loss for each subject (patch) separately\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Test Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb4d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b74c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
